---
title: "tidymodels_example"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidymodels)
library(readr)
library(vip)
```

## Predictive modeling case study for tidymodels package

This case study brings all the different components from the getstarted tutorials together. Hopefully this will be useful to walk through and then reference in the future when I'm building my own models.

```{r loading data}
hotels <- 
  read_csv('https://tidymodels.org/start/case-study/hotels.csv') %>%
  mutate_if(is.character, as.factor) 

glimpse(hotels)
```
We are building a model to predict when hotel stays included chilren.

```{r}
hotels %>% count(children) %>%
  mutate(prop = n/sum(n))
#note that this is imbalanced. Rather than upsampling or down sampling we are going to just go ahead as is.

set.seed(123)
splits <- initial_split(hotels, strata = children)
hotel_other <-  training(splits)
hotel_test <- testing(splits)

#rather than doing folded CV, we are going to use a single validation set. Banking on the idea that the single validation set is large enough to give us a good estimate of actual model performance.
val_set <- validation_split(hotel_other, 
                            strata = children, 
                            prop = 0.80)
val_set

```

# First model: penalized logistic regression
We want our model to do feature selection during training. We are going to use the glmnet package for this, which uses penalized maximum likelihood to fit model. This is a package for implementing lasso or ridge regression. Interestingly can set a mixture between the two in the logistic_reg function, with mixture =1 being pure lasso.

```{r setting up LR model}
lr_mod <- logistic_reg(penalty = tune(),mixture=1) %>%
  set_engine("glmnet")

holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- 
  recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>% 
  step_rm(arrival_date) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

lr_workflow <-  workflow() %>%
  add_model(lr_mod)%>%
  add_recipe(lr_recipe)

lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

```

```{r LR training and tuning}
lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot 

```
Here we see that the inflection point is around .00137, and so we are going to choose that value for out tuned hyperparameter as it will minimize the number of predictors without losing accuracy.

```{r}
lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(12)#this is hard coded in, which isn't ideal
lr_best

lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```

# Second model: tree-based ensemble
aside: the ranger package can be used to compute individual random forest models in parallel even if you are using a single validation set.
**do not do this in most contexts, it's usually better to let the tune package handle this if you are using any other resampling method like cross validation**
```{r setup RF model}
cores <- parallel::detectCores()
cores

rf_mod <- rand_forest(mtry = tune(),min_n = tune(), trees = 500)%>% #n trees was 1000 before 
  set_engine("ranger", num.threads = cores) %>%
  set_mode("classification")

rf_recipe <-  recipe(children ~., data = hotel_other) %>%
  step_date(arrival_date) %>%
  step_holiday(arrival_date) %>%
  step_rm(arrival_date)

rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)
```
It takes a fairly long time to do model tuning on this, because you have to run N iterations each of which takes my computer a while. I ended up reducing the number of tuning points and number of trees per forest to make it run a little faster and so I won't get as good of results as the tutorial probably.
```{r train and tune RF model}
set.seed(345)
rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid =5, #25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

autoplot(rf_res)
rf_res %>% 
  show_best(metric = "roc_auc")
rf_best <- 
  rf_res %>% 
  select_best(metric = "roc_auc")

rf_res %>% 
  collect_predictions()

rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Random Forest")

bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)
```

# The last model
```{r last model setup}
last_rf_mod <- 
  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% #note the hardcoded params, also probably not ideal
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit %>%
  collect_metrics()

last_rf_fit %>% 
  pluck(".workflow", 1) %>%   #new thing here, plucking out the first workflow
  pull_workflow_fit() %>% 
  vip(num_features = 20)

last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(children, .pred_children) %>% 
  autoplot()
```

